{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68af8c6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T06:59:54.510854500Z",
     "start_time": "2023-09-26T06:59:54.480145300Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/user/xgf/Disstill_defect_interpretation/Disstill_defect_interpretation2/models/utils.py:18: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from models import ConvNet, SoftBinaryDecisionTree\n",
    "from models.utils import brand_new_tfsession, draw_tree\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "\n",
    "sess = brand_new_tfsession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db2a5b59",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88, 13, 2, 1) (88, 1) (10, 13, 2, 1) (10, 1) (7, 13, 2, 1) (7, 1)\n"
     ]
    }
   ],
   "source": [
    "#上采样\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import *\n",
    "# from keras.layers import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import xlwt\n",
    "\n",
    "def normalization(data):\n",
    "    _range = np.max(data) - np.min(data)\n",
    "    return (data - np.min(data)) / _range\n",
    "\n",
    "original_data = pd.read_csv('./dataset/SOFTLAB/ar3.csv')\n",
    "# original_data = pd.read_csv('./dataset/bugzilla.csv')\n",
    "original_data.isnull().values.any()  # Gives false ie:No null value in dataset\n",
    "original_data = original_data.fillna(value=False)  #将缺失值填充为False\n",
    "# original_Y = original_data['defects']  #Defective   class   isDefective  defects\n",
    "original_Y = original_data['defects']\n",
    "original_Y = pd.DataFrame(original_Y)\n",
    "original_data = normalization(original_data)\n",
    "\n",
    "#  将数据写入新文件  \n",
    "#original_data.to_excel('C:/Users/lenovo/Desktop/excel/ar6.xls',sheet_name='ar6',index=False)\n",
    "# original_X = pd.DataFrame(original_data.drop(['defects'], axis=1))  #Defective   class  isDefective  defects\n",
    "original_X = pd.DataFrame(original_data.drop(['defects'], axis=1))\n",
    "#print(original_X)\n",
    "#分为训练集和测试集\n",
    "x_train, x_test, y_train, y_test = train_test_split(original_X, original_Y, test_size=.1, random_state=12)\n",
    "# now we resample, and from that we take training and validation sets\n",
    "sm = SMOTE(random_state=12, sampling_strategy=1.0) # 解决分类不平衡问题\n",
    "x, y = sm.fit_resample(x_train, y_train)\n",
    "y_train = pd.DataFrame(y, columns=['defects']) #Defective  class  isDefective  defects\n",
    "x_train = pd.DataFrame(x, columns=original_X.columns)\n",
    "#print(x_test) \n",
    "\n",
    "#对单个数据进行加量 \n",
    "#x_test.loc[1,'total_loc'] =  0.264528801 \n",
    "#x_test.loc[1,'comment_loc'] =  0.337140831 \n",
    "#x_test.loc[1,'executable_loc'] =  0.203275228 \n",
    "#x_test.loc[1,'unique_operands'] =  0.369565217 \n",
    "#x_test.loc[1,'unique_operators'] =  0.479770399 \n",
    "#x_test.loc[1,'total_operands'] =  0.282879173 \n",
    "#x_test.loc[1,'total_operators'] =  0.250517298 \n",
    "#x_test.loc[1,'halstead_vocabulary'] =  0.414459875 \n",
    "#x_test.loc[1,'halstead_length'] =  0.262842023 \n",
    "#x_test.loc[1,'halstead_volume'] =  0.244372278 \n",
    "#x_test.loc[1,'halstead_level'] =  0.500382013 \n",
    "#x_test.loc[1,'halstead_difficulty'] =  0.280770116 \n",
    "#x_test.loc[1,'halstead_effort'] =  0.152186111 \n",
    "#x_test.loc[1,'halstead_time'] =  0.152185296 \n",
    "#x_test.loc[1,'branch_count'] =  0.172317183 \n",
    "#x_test.loc[1,'condition_count'] =  0.171775777 \n",
    "#x_test.loc[1,'cyclomatic_complexity'] =  0.179914274 \n",
    "#x_test.loc[1,'cyclomatic_density'] =  0.646325185 \n",
    "#x_test.loc[1,'decision_density'] =  0.290946038 \n",
    "#x_test.loc[1,'design_density'] =  0.200977818 \n",
    "#x_test.loc[1,'normalized_cyclomatic_complexity'] =  0.455357667 \n",
    "\n",
    "#细分出验证集\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=.1, random_state=12)\n",
    "\n",
    "x_train = x_train.values\n",
    "x_val = x_val.values\n",
    "x_test = x_test.values\n",
    "y_train = y_train.values\n",
    "y_val = y_val.values\n",
    "y_test = y_test.values\n",
    "\n",
    "img_rows, img_cols = 13,2 #7 3   13,2\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b58f0aa",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 2 1 2\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols, img_chans, n_classes = 13,2, 1, 2  #7,3   13,2\n",
    "print(img_rows, img_cols, img_chans, n_classes)\n",
    "\n",
    "# retrieve image and label shapes from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b4d6e8",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88, 2) (10, 2) (7, 2)\n"
     ]
    }
   ],
   "source": [
    "# convert labels to 1-hot vectors\n",
    "y_train = tf.keras.utils.to_categorical(y_train, n_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, n_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, n_classes)\n",
    "\n",
    "print(y_train.shape, y_val.shape, y_test.shape)\n",
    "#print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "384884ba",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# normalize inputs and cast to float\n",
    "x_train = (x_train / np.max(x_train)).astype(np.float32)\n",
    "x_val = (x_val / np.max(x_val)).astype(np.float32)\n",
    "x_test = (x_test / np.max(x_test)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c1d9231",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model from assets/teacher_model.hdf5.\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/guifangTf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "accuracy: 90.91% | loss: 0.2738029252399098\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "batch_size = 2\n",
    "# data_valid = (x_val, y_val)\n",
    "nn = ConvNet(img_rows, img_cols, img_chans, n_classes)\n",
    "nn.maybe_train(data_train=(x_train, y_train),\n",
    "               data_valid=(x_val, y_val),\n",
    "               batch_size=4, epochs=40)  #batch_size=4 2\n",
    "# nn.maybe_train(x_train, y_train, validation_data=data_valid,batch_size=batch_size, epochs=epochs)\n",
    "nn.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b76afadd",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 90.00% | loss: 0.34726986289024353\n",
      "accuracy: 100.00% | loss: 0.2258363515138626\n"
     ]
    }
   ],
   "source": [
    "nn.evaluate(x_val, y_val)\n",
    "nn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5f1bcb6",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 2)\n"
     ]
    }
   ],
   "source": [
    "y_train_soft = nn.predict(x_train)\n",
    "# print(y_train_soft)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25efdc66",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 1.0\n",
      "precision= 1.0\n",
      "recall= 1.0\n",
      "f-score= 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y = nn.predict(x_test)\n",
    "\n",
    "y = np.argmax(y,axis=1)\n",
    "y_test = np.argmax(y_test,axis=1)\n",
    "\n",
    "accuracy_score=metrics.accuracy_score(y_test,y)\n",
    "\n",
    "print(\"accuracy_score\",accuracy_score)\n",
    "\n",
    "precision=metrics.precision_score(y_test,y,average='macro')\n",
    "\n",
    "print(\"precision=\",precision)#precision对比可以，超过图中方法\n",
    "\n",
    "recall=metrics.recall_score(y_test,y,average='macro')\n",
    "print(\"recall=\",recall)#recall比其他两种方法都要低\n",
    "\n",
    "fscore=metrics.f1_score(y_test,y,average='macro')\n",
    "print(\"f-score=\",fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d6b8318",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((88, 26), (10, 26), (7, 26))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_flat = x_train.reshape((x_train.shape[0], -1))\n",
    "x_val_flat = x_val.reshape((x_val.shape[0], -1))\n",
    "x_test_flat = x_test.reshape((x_test.shape[0], -1))\n",
    "\n",
    "x_train_flat.shape, x_val_flat.shape, x_test_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1864e3ae",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "n_features = img_rows * img_cols * img_chans\n",
    "tree_depth = 4\n",
    "penalty_strength = 1e+1 \n",
    "penalty_decay = 0.25\n",
    "ema_win_size = 1000\n",
    "inv_temp = 0.01\n",
    "learning_rate = 1e-06  \n",
    "batch_size = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e6676c2",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/user/xgf/Disstill_defect_interpretation/Disstill_defect_interpretation2/models/tree.py:18: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/guifangTf2/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:6094: calling map_fn (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "Built tree has 16 leaves out of 31 nodes\n"
     ]
    }
   ],
   "source": [
    "sess = brand_new_tfsession(sess)\n",
    "\n",
    "tree1 = SoftBinaryDecisionTree(tree_depth, n_features, n_classes,\n",
    "    penalty_strength=penalty_strength, penalty_decay=penalty_decay,\n",
    "    inv_temp=inv_temp, ema_win_size=ema_win_size, learning_rate=learning_rate)\n",
    "tree1.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c602c5a7",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model from assets/non-distilled/tree-model.\n",
      "assets/non-distilled/tree-model is not a valid checkpoint. Training from scratch.\n",
      "Train on 88 samples, validate on 10 samples\n",
      "Epoch 1/40\n",
      "88/88 [==============================] - 1s 9ms/sample - loss: inf - acc: 0.2727 - val_loss: 19.4490 - val_acc: 0.6000\n",
      "Epoch 2/40\n",
      "88/88 [==============================] - 0s 4ms/sample - loss: 17.5267 - acc: 0.2727 - val_loss: 16.4777 - val_acc: 0.6000\n",
      "Epoch 3/40\n",
      "88/88 [==============================] - 0s 5ms/sample - loss: 16.2461 - acc: 0.2841 - val_loss: 16.2871 - val_acc: 0.6000\n",
      "Epoch 4/40\n",
      "88/88 [==============================] - 0s 4ms/sample - loss: 16.6486 - acc: 0.2841 - val_loss: 17.1341 - val_acc: 0.6000\n",
      "Epoch 5/40\n",
      "88/88 [==============================] - 0s 4ms/sample - loss: 17.7821 - acc: 0.3182 - val_loss: 18.4960 - val_acc: 0.5000\n",
      "Epoch 6/40\n",
      "88/88 [==============================] - 0s 4ms/sample - loss: 19.3034 - acc: 0.3295 - val_loss: 20.1483 - val_acc: 0.5000\n",
      "Epoch 7/40\n",
      "88/88 [==============================] - 0s 4ms/sample - loss: 21.0513 - acc: 0.3295 - val_loss: 21.9765 - val_acc: 0.5000\n",
      "Epoch 8/40\n",
      "88/88 [==============================] - 0s 4ms/sample - loss: 22.9397 - acc: 0.3295 - val_loss: 23.9163 - val_acc: 0.5000\n",
      "Epoch 9/40\n",
      "88/88 [==============================] - 0s 4ms/sample - loss: 24.9184 - acc: 0.3295 - val_loss: 25.9291 - val_acc: 0.5000\n",
      "Epoch 10/40\n",
      "88/88 [==============================] - 0s 4ms/sample - loss: 26.9569 - acc: 0.3295 - val_loss: 27.9909 - val_acc: 0.5000\n",
      "Epoch 11/40\n",
      "88/88 [==============================] - 0s 4ms/sample - loss: 29.0374 - acc: 0.3295 - val_loss: 30.0851 - val_acc: 0.5000\n",
      "Epoch 12/40\n",
      "88/88 [==============================] - 0s 4ms/sample - loss: 31.1434 - acc: 0.3295 - val_loss: 32.2019 - val_acc: 0.5000\n",
      "Epoch 13/40\n",
      "88/88 [==============================] - 0s 4ms/sample - loss: 33.2700 - acc: 0.3295 - val_loss: 34.3331 - val_acc: 0.5000\n",
      "Epoch 14/40\n",
      "88/88 [==============================] - 0s 4ms/sample - loss: 35.4073 - acc: 0.3295 - val_loss: 36.4720 - val_acc: 0.5000\n",
      "Epoch 15/40\n",
      "88/88 [==============================] - 0s 4ms/sample - loss: 37.5459 - acc: 0.3295 - val_loss: 38.6211 - val_acc: 0.5000\n",
      "Epoch 16/40\n",
      "88/88 [==============================] - 0s 5ms/sample - loss: 39.6945 - acc: 0.3295 - val_loss: 40.7677 - val_acc: 0.5000\n",
      "Epoch 17/40\n",
      "88/88 [==============================] - 0s 4ms/sample - loss: 41.8419 - acc: 0.3295 - val_loss: 42.9022 - val_acc: 0.5000\n",
      "Epoch 18/40\n",
      "88/88 [==============================] - 0s 4ms/sample - loss: 43.9717 - acc: 0.3295 - val_loss: 45.0156 - val_acc: 0.5000\n",
      "Epoch 19/40\n",
      "88/88 [==============================] - 0s 4ms/sample - loss: 46.0946 - acc: 0.3295 - val_loss: 47.1294 - val_acc: 0.5000\n",
      "Epoch 20/40\n",
      "88/88 [==============================] - 0s 4ms/sample - loss: 48.1610 - acc: 0.3295 - val_loss: 49.1495 - val_acc: 0.5000\n",
      "Epoch 21/40\n",
      "88/88 [==============================] - 0s 4ms/sample - loss: 50.1484 - acc: 0.3295 - val_loss: 51.1608 - val_acc: 0.5000\n",
      "Epoch 00021: early stopping\n",
      "Saving trained model to assets/non-distilled/tree-model.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "epochs = 40\n",
    "\n",
    "es = EarlyStopping(monitor='val_acc', patience=20, verbose=1)\n",
    "\n",
    "tree1.maybe_train(\n",
    "    sess=sess, data_train=(x_train_flat, y_train), data_valid=(x_val_flat, y_val),\n",
    "    batch_size=batch_size, epochs=epochs, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f86ba029",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 50.00% | loss: 51.28951797485352\n",
      "accuracy: 28.57% | loss: 38.48189163208008\n"
     ]
    }
   ],
   "source": [
    "tree1.evaluate(x=x_val_flat, y=y_val, batch_size=2)\n",
    "tree1.evaluate(x=x_test_flat, y=y_test, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41e32c18",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built tree has 16 leaves out of 31 nodes\n"
     ]
    }
   ],
   "source": [
    "sess = brand_new_tfsession(sess)\n",
    "\n",
    "tree1 = SoftBinaryDecisionTree(tree_depth, n_features, n_classes,\n",
    "    penalty_strength=penalty_strength, penalty_decay=penalty_decay,\n",
    "    inv_temp=inv_temp, ema_win_size=ema_win_size, learning_rate=learning_rate)\n",
    "tree1.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ed46941",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model from assets/distilled/tree-model.\n",
      "INFO:tensorflow:Restoring parameters from assets/distilled/tree-model\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "\n",
    "es = EarlyStopping(monitor='val_acc', patience=20, verbose=1)\n",
    "\n",
    "tree1.maybe_train(\n",
    "    sess=sess, data_train=(x_train_flat, y_train_soft), data_valid=(x_val_flat, y_val),\n",
    "    batch_size=batch_size, epochs=epochs, callbacks=[es], distill=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd7d3293",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 60.00% | loss: 39.723823928833006\n",
      "accuracy: 57.14% | loss: 24.434832436697825\n"
     ]
    }
   ],
   "source": [
    "tree1.evaluate(x=x_val_flat, y=y_val, batch_size=1)\n",
    "\n",
    "tree1.evaluate(x=x_test_flat, y=y_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28f68514",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4806271655352e-07\n",
      "-3.599453538290518e-07\n",
      "-7.690680047896341e-07\n",
      "-3.374539856507957e-07\n",
      "3.0006822402911303e-07\n",
      "-3.56190292034977e-07\n",
      "3.653482248065968e-07\n",
      "3.2172538418763314e-07\n",
      "0.0\n",
      "0.0\n",
      "-2.9000535936196956e-07\n",
      "-3.7189962000576484e-07\n",
      "4.991842260342794e-07\n",
      "4.991844536173576e-07\n",
      "8.514480284743533e-07\n",
      "0.0\n",
      "0.0\n",
      "-3.3225452017003854e-07\n",
      "2.5214147702874236e-07\n",
      "-3.6501322738325167e-07\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#度量元灵敏度计算\n",
    "\n",
    "#增量为方差\n",
    "loss   = 12.62952525799091   #53.85\n",
    "loss1  = 12.629525331350473\n",
    "loss2  = 12.629525184631348\n",
    "loss3  = 12.629525111271786\n",
    "loss4  = 12.629525184631348\n",
    "loss5  = 12.629525331350473\n",
    "loss6  = 12.629525184631348\n",
    "loss7  = 12.629525331350473\n",
    "loss8  = 12.629525331350473\n",
    "loss9  = 12.62952525799091\n",
    "loss10 = 12.62952525799091\n",
    "loss11 = 12.629525184631348\n",
    "loss12 = 12.629525184631348\n",
    "loss13 = 12.629525331350473\n",
    "loss14 = 12.629525331350473\n",
    "loss15 = 12.629525404710035\n",
    "loss16 = 12.62952525799091\n",
    "loss17 = 12.62952525799091\n",
    "loss18 = 12.629525184631348\n",
    "loss19 = 12.629525331350473    #46.15% \n",
    "loss20 = 12.629525184631348\n",
    "loss21 = 12.62952525799091\n",
    "\n",
    "Sensitivity1 = (loss1 - loss) / 0.21076536\n",
    "Sensitivity2 = (loss2 - loss) / 0.203807498\n",
    "Sensitivity3 = (loss3 - loss) / 0.190775228\n",
    "Sensitivity4 = (loss4 - loss) / 0.217391304\n",
    "Sensitivity5 = (loss5 - loss) / 0.244476282\n",
    "Sensitivity6 = (loss6 - loss) / 0.205956096\n",
    "Sensitivity7 = (loss7 - loss) / 0.200793541\n",
    "Sensitivity8 = (loss8 - loss) / 0.228019197\n",
    "Sensitivity9 = (loss9 - loss) / 0.202439338\n",
    "Sensitivity10 = (loss10 - loss) / 0.199596158\n",
    "Sensitivity11 = (loss11 - loss) / 0.252959332\n",
    "Sensitivity12 = (loss12 - loss) / 0.197256351\n",
    "Sensitivity13 = (loss13 - loss) / 0.146958898\n",
    "Sensitivity14 = (loss14 - loss) / 0.146958831\n",
    "Sensitivity15 = (loss15 - loss) / 0.172317183\n",
    "Sensitivity16 = (loss16 - loss) / 0.171775777\n",
    "Sensitivity17 = (loss17 - loss) / 0.179914274\n",
    "Sensitivity18 = (loss18 - loss) / 0.22079327\n",
    "Sensitivity19 = (loss19 - loss) / 0.290946038\n",
    "Sensitivity20 = (loss20 - loss) / 0.200977818\n",
    "Sensitivity21 = (loss21 - loss) / 0.221315114\n",
    "\n",
    "print(Sensitivity1)\n",
    "print(Sensitivity2)\n",
    "print(Sensitivity3)\n",
    "print(Sensitivity4)\n",
    "print(Sensitivity5)\n",
    "print(Sensitivity6)\n",
    "print(Sensitivity7)\n",
    "print(Sensitivity8)\n",
    "print(Sensitivity9)\n",
    "print(Sensitivity10)\n",
    "print(Sensitivity11)\n",
    "print(Sensitivity12)\n",
    "print(Sensitivity13)\n",
    "print(Sensitivity14)\n",
    "print(Sensitivity15)\n",
    "print(Sensitivity16)\n",
    "print(Sensitivity17)\n",
    "print(Sensitivity18)\n",
    "print(Sensitivity19)\n",
    "print(Sensitivity20)\n",
    "print(Sensitivity21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c957438",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-32dd4582a5cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# 生成解释器\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m explainer = lime.lime_tabular.LimeTabularExplainer(x_train_flat, feature_names=feature_names,\n\u001b[0;32m---> 18\u001b[0;31m                                                    class_names=['false','true'], discretize_continuous=True)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m# 对局部点的解释\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_flat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/guifangTf2/lib/python3.7/site-packages/lime/lime_tabular.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, training_data, mode, training_labels, feature_names, categorical_features, categorical_names, kernel_width, kernel, verbose, class_names, feature_selection, discretize_continuous, discretizer, sample_around_instance, random_state, training_data_stats)\u001b[0m\n\u001b[1;32m    216\u001b[0m                         \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m                         random_state=self.random_state)\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mdiscretizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'decile'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 self.discretizer = DecileDiscretizer(\n",
      "\u001b[0;32m~/anaconda3/envs/guifangTf2/lib/python3.7/site-packages/lime/discretize.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, categorical_features, feature_names, labels, random_state)\u001b[0m\n\u001b[1;32m    178\u001b[0m         BaseDiscretizer.__init__(self, data, categorical_features,\n\u001b[1;32m    179\u001b[0m                                  \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                                  random_state=random_state)\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/guifangTf2/lib/python3.7/site-packages/lime/discretize.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, categorical_features, feature_names, labels, random_state, data_stats)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mn_bins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Actually number of borders (= #bins-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mboundaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'%s <= %.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#####lime\n",
    "\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "#ar1\n",
    "feature_names = 'total_loc,comment_loc,executable_loc,unique_operands,unique_operators,total_operands,total_operators,halstead_vocabulary,\\\n",
    "                 halstead_length,halstead_volume,halstead_level,halstead_difficulty,halstead_effort,halstead_time,branch_count,\\\n",
    "                 condition_count,cyclomatic_complexity,cyclomatic_density,decision_density,design_density,\\\n",
    "                 normalized_cyclomatic_complexity'.split(',')\n",
    "\n",
    "# 生成解释器\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(x_train_flat, feature_names=feature_names,\n",
    "                                                   class_names=['false','true'], discretize_continuous=True)\n",
    "# 对局部点的解释\n",
    "i = np.random.randint(0, x_test_flat.shape[0])\n",
    "exp = explainer.explain_instance(x_test_flat[i], tree1.predict_on_batch, num_features=16)\n",
    "# 显示详细信息图\n",
    "exp.show_in_notebook(show_table=True, show_all=True)\n",
    "# 显示权重图\n",
    "exp.as_pyplot_figure()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
